\chapter{Multiple Systems}
\section{Classical Information}
\subsection{Classical states}
Suppose that we have two systems:
\begin{itemize}
    \item $\mathsf{X}$ is a system having classical state set $\Sigma$.
    \item $\mathsf{Y}$ is a system having classical state set $\Gamma$.
\end{itemize}
Imagine that $\mathsf{X}$ and $\mathsf{Y}$ are placed side-by-side, with $\mathsf{X}$ on the left and $\mathsf{Y}$ on the right, and viewed together as if they form a single system.

\bigbreak

We denote this new compound system by $(\mathsf{X},\mathsf{Y})$, or $\mathsf{XY}$.

\qss{}{What are the classical states of $(\mathsf{X},\mathsf{Y})$?}
\begin{sol}
    The classical state set of $(\mathsf{X},\mathsf{Y})$ is the \textbf{Cartesian product}:
    \begin{equation*}
        \Sigma \times \Gamma = \{(\mathsf{x},\mathsf{y}) \mid \mathsf{x} \in \Sigma, \mathsf{y} \in \Gamma\}
    \end{equation*}
\end{sol}
\remark{It's not important that $\mathsf{X}$ is on the left and $\mathsf{Y}$ is on the right; we could have just as well placed $\mathsf{Y}$ on the left and $\mathsf{X}$ on the right. The important thing is that the two systems are distinguishable.}

\ex{Card suits}{
    If $\Sigma=\{0,1\}$ and $\Gamma=\{\clubsuit,\diamondsuit,\heartsuit,\spadesuit\}$, then:
    \begin{equation*}
        \Sigma \times \Gamma = \{(0,\clubsuit),(0,\diamondsuit),(0,\heartsuit),(0,\spadesuit),(1,\clubsuit),(1,\diamondsuit),(1,\heartsuit),(1,\spadesuit)\}
    \end{equation*}
}

\raggedright
This description generalizes to more than two systems in a natural way.

\bigbreak

Suppose $\mathsf{X}_1,\cdots,\mathsf{X}_n$ are systems having classical state sets $\Sigma_1,\cdots,\Sigma_n$, respectively. 

\bigbreak

The classical state set of the $n$-tuple ($\mathsf{X}_1,\cdots,\mathsf{X}_n$), viewed as a single compound system, is the Cartesian product:
\begin{equation*}
    \Sigma_1 \times \cdots \times \Sigma_n = \{(\mathsf{x}_1,\cdots,\mathsf{x}_n) \mid \mathsf{x}_1 \in \Sigma_1, \cdots, \mathsf{x}_n \in \Sigma_n\}
\end{equation*}

\newpage

\ex{}{
    If $\Sigma_1=\Sigma_2=\Sigma_3=\{0,1\}$, then the classical state set of ($\mathsf{X}_1,\mathsf{X}_2,\mathsf{X}_3$) is:
    \begin{equation*}
        \Sigma_1 \times \Sigma_2 \times \Sigma_3 = \{(0,0,0),(0,0,1),(0,1,0),(0,1,1),(1,0,0),(1,0,1),(1,1,0),(1,1,1)\}
    \end{equation*}
}

An $n$-tuple $(\mathsf{x}_1,\cdots,\mathsf{x}_n)$ may also be written as a string $\mathsf{x}_1\cdots \mathsf{x}_n$.

\ex{Binary alphabet}{
    Suppose $\mathsf{X}_1,\cdots,\mathsf{X}_n$ are bits, so thei classical state sets are all the same:
    \begin{equation*}
        \Sigma_1=\Sigma_2=\cdots=\Sigma_{10}=\{0,1\}
    \end{equation*}
    The classical state set of ($\mathsf{X}_1,\cdots,\mathsf{X}_n$) is the Cartesian product:
    \begin{equation*}
        \Sigma_1\times\Sigma_2\times\cdots\times\Sigma_{10}=\{0,1\}^{10}
    \end{equation*}

    You can also think about this as a 10-bit register in a classical computer. Written as strings, these classical states look like this:
    \begin{equation*}
        \begin{matrix}
            0000000000 \\
            0000000001 \\
            0000000010 \\
            0000000011 \\
            \vdots \\
            1111111111
        \end{matrix}
    \end{equation*}
}

\nt{
    Cartesian products of classical state sets are ordered lexicographically (i.e., in dictionary order):
    \begin{itemize}
        \item We assume the individual classical state sets are already ordered.
        \item Significance decreases from left to right.
    \end{itemize}
}

\ex{}{
    The Cartesian product $\{1,2,3\}\times\{0,1\}$ is ordered like this:
    \begin{equation*}
        (1,0),(1,1),(2,0),(2,1),(3,0),(3,1)
    \end{equation*}
}

When $n$-tuples are written as strings and ordered in this way, we observe familiar patterns, such as $\{0,1\}\times\{0,1\}$ being ordered as $00,01,10,11$.

\subsection{Probabilistic states}
Probabilistic states of compound systems associates probabilities with the Cartesian product of the classical state sets of individual systems.

\ex{}{
    This is a probabilistic state pair of bits $(\mathsf{X},\mathsf{Y})$:
    \begin{center}
        $Pr((\mathsf{X},\mathsf{Y})=(0,0))=\dfrac{1}{2}$\\[1em]
        $Pr((\mathsf{X},\mathsf{Y})=(0,1))=0$\\[1em]
        $Pr((\mathsf{X},\mathsf{Y})=(1,0))=0$\\*[1em]
        $Pr((\mathsf{X},\mathsf{Y})=(1,1))=\dfrac{1}{2}$
    \end{center}

    An alternate notation using vector notation:
    \begin{equation*}
        \begin{pmatrix}
            \dfrac{1}{2} \\[1em]
            0 \\[1em]
            0 \\[1em]
            \dfrac{1}{2}
            \end{pmatrix}
            \qquad
            \begin{aligned}
            \leftarrow\text{ probability associated with state 00} \\[1em]
            \leftarrow\text{ probability associated with state 01} \\[1em]
            \leftarrow\text{ probability associated with state 10} \\[1em]
            \leftarrow\text{ probability associated with state 11}
            \end{aligned}
    \end{equation*}
}

For a given probabilistic state of $(\mathsf{X},\mathsf{Y})$, we say that $\mathsf{X}$ and $\mathsf{Y}$ are \textbf{statistically independent} if
\begin{equation*}
    Pr((\mathsf{X},\mathsf{Y})=(\mathsf{x},\mathsf{y}))=Pr(\mathsf{X}=\mathsf{x})Pr(\mathsf{Y}=\mathsf{y})
\end{equation*}
for all $\mathsf{x}\in\Sigma$ and $\mathsf{y}\in\Gamma$.

\bigbreak

Suppose that a probabilistic state of $(\mathsf{X},\mathsf{Y})$ is expressed as a vector:
\begin{equation*}
    \ket{\pi}=\sum\limits_{(\mathsf{x},\mathsf{y})\in\Sigma\times\Gamma}p_{ab}\ket{\mathsf{xy}}
\end{equation*}
The systems $\mathsf{X}$ and $\mathsf{Y}$ are independent if there exist probability vectors
\begin{equation*}
    \ket{\phi}=\sum\limits_{\mathsf{x}\in\Sigma}q_x\ket{\mathsf{x}} \quad \text{and} \quad \ket{\psi}=\sum\limits_{\mathsf{y}\in\Gamma}r_y\ket{\mathsf{y}}
\end{equation*}
such that $p_{\mathsf{xy}}=q_xr_y$ for all $\mathsf{x}\in\Sigma$ and $\mathsf{y}\in\Gamma$.

\ex{Independent bits}{
    The probabilistic state of a pair of bits $(\mathsf{X},\mathsf{Y})$ represented by the vector
    \begin{equation*}
        \ket{\pi}=\dfrac{1}{6}\ket{00}+\dfrac{1}{12}\ket{01}+\dfrac{1}{2}\ket{10}+\dfrac{1}{4}\ket{11}
    \end{equation*}
    is one in which $\mathsf{X}$ and $\mathsf{Y}$ are independent. The required condition is true for these probability vectors:
    \begin{equation*}
        \ket{\phi}=\dfrac{1}{4}\ket{0}+\dfrac{3}{4}\ket{1} \quad \text{and} \quad \ket{\psi}=\dfrac{2}{3}\ket{0}+\dfrac{1}{3}\ket{1}
    \end{equation*}
}

\ex{Dependent bits}{
    For the probabilistic state
    \begin{equation*}
        \dfrac{1}{2}\ket{00}+\dfrac{1}{2}\ket{11}
    \end{equation*}
of two bits $(\mathsf{X},\mathsf{Y})$, we have that $\mathsf{X}$ and $\mathsf{Y}$ are not independent.

\bigbreak

If they were, we would have numbers $q_0,q_1,r_0,r_1$ such that
\begin{center}
    $q_0r_0=\dfrac{1}{2}$\\[1em]
    $q_0r_1=0$\\[1em]
    $q_1r_0=0$\\*[1em]
    $q_1r_1=\dfrac{1}{2}$
\end{center}
But if $q_0r_1=0$, then either $q_0=0$ or $r_1=0$ (or both), contradicting either the first or last equality since any number multiplied by 0 is 0.
}

\subsection{Tensor products of vectors}
The tensor product of two vectors
\begin{equation*}
    \ket{\phi}=\sum\limits_{\mathsf{x}\in\Sigma}\alpha_x\ket{\mathsf{x}} \quad \text{and} \quad \ket{\psi}=\sum\limits_{\mathsf{y}\in\Gamma}\beta_y\ket{\mathsf{y}}
\end{equation*}
is the vector
\begin{equation*}
    \ket{\phi}\otimes\ket{\psi}=\sum\limits_{(\mathsf{x},\mathsf{y})\in\Sigma\times\Gamma}\alpha_x\beta_y\ket{\mathsf{xy}}
\end{equation*}
Equivalently, the vector $\ket{\phi}\otimes\ket{\psi}$ is defined by this condition:
\begin{equation*}
    \braket{\mathsf{xy}\mid\pi}=\braket{\mathsf{x}\mid\phi}\braket{\mathsf{y}\mid\psi} \qquad \text{(for all } \mathsf{x}\in\Sigma \text{ and } \mathsf{y}\in\Gamma \text{)}
\end{equation*}

\remark{In essense, this is the same operation for probabilistic states of a pair of independent bits; we are just giving a name to it now.}

Following our convention for ordering the elements of Cartesian product sets, we obtain this specification for the tensor product of two column vectors:
\begin{equation*}
    \begin{pmatrix}
        \alpha_1 \\
        \vdots \\
        \alpha_m
    \end{pmatrix}
    \otimes
    \begin{pmatrix}
        \beta_1 \\
        \vdots \\
        \beta_k
    \end{pmatrix}
    =
    \begin{pmatrix}
        \alpha_1\beta_1 \\
        \vdots \\
        \alpha_1\beta_k \\
        \alpha_2\beta_1 \\
        \vdots \\
        \alpha_2\beta_k \\
        \vdots \\
        \alpha_m\beta_1 \\
        \vdots \\
        \alpha_m\beta_k
    \end{pmatrix}
\end{equation*}

\ex{Tensor product}{
    \begin{center}
        $\begin{pmatrix}
            \alpha_1 \\
            \alpha_2 \\
            \alpha_3
        \end{pmatrix}
        \otimes
        \begin{pmatrix}
            \beta_1 \\
            \beta_2 \\
            \beta_3 \\
            \beta_4
        \end{pmatrix}
        =$
        $\begin{aligned}
            \alpha_1
            \begin{pmatrix}
                \beta_1 \\
                \beta_2 \\
                \beta_3 \\
                \beta_4
            \end{pmatrix}
            \\
            \alpha_2
            \begin{pmatrix}
                \beta_1 \\
                \beta_2 \\
                \beta_3 \\
                \beta_4
            \end{pmatrix}
            \\
            \alpha_3
            \begin{pmatrix}
                \beta_1 \\
                \beta_2 \\
                \beta_3 \\
                \beta_4
            \end{pmatrix}
        \end{aligned}$
        $=$
        $\begin{aligned}
            \begin{pmatrix}
                \alpha_1\beta_1 \\
                \alpha_1\beta_2 \\
                \alpha_1\beta_3 \\
                \alpha_1\beta_4
            \end{pmatrix}
            \\
            \begin{pmatrix}
                \alpha_2\beta_1 \\
                \alpha_2\beta_2 \\
                \alpha_2\beta_3 \\
                \alpha_2\beta_4
            \end{pmatrix}
            \\
            \begin{pmatrix}
                \alpha_3\beta_1 \\
                \alpha_3\beta_2 \\
                \alpha_3\beta_3 \\
                \alpha_3\beta_4
            \end{pmatrix}
        \end{aligned}$
        $=
        \begin{pmatrix}
            \alpha_1\beta_1 \\
            \alpha_1\beta_2 \\
            \alpha_1\beta_3 \\
            \alpha_1\beta_4 \\
            \alpha_2\beta_1 \\
            \alpha_2\beta_2 \\
            \alpha_2\beta_3 \\
            \alpha_2\beta_4 \\
            \alpha_3\beta_1 \\
            \alpha_3\beta_2 \\
            \alpha_3\beta_3 \\
            \alpha_3\beta_4
        \end{pmatrix}$
    \end{center}
}

\newpage

Observe the following expression for tensor products of standard basis vectors:
\begin{equation*}
    \ket{\mathsf{x}}\otimes\ket{\mathsf{y}}=\ket{\mathsf{x}}\ket{\mathsf{y}}=\ket{\mathsf{xy}}
\end{equation*}
Alternatively, writing $(\mathsf{x},\mathsf{y})$ as an ordered pair rather than a string, we could write
\begin{equation*}
    \ket{\mathsf{x}}\otimes\ket{\mathsf{y}}=\ket{(\mathsf{x},\mathsf{y})}
\end{equation*}
but it is more common to write
\begin{equation*}
    \ket{\mathsf{x}}\otimes\ket{\mathsf{y}}=\ket{\mathsf{x},\mathsf{y}}
\end{equation*}

\nt{
    The tensor product of two vectors is \textbf{bilinear}.
    \begin{enumerate}
        \item Linearity in the first argument:
        \begin{center}
            $(\ket{\phi_1}+\ket{\phi_2})\otimes\ket{\psi}=\ket{\phi_1}\otimes\ket{\psi}+\ket{\phi_2}\otimes\ket{\psi}$\\[1em]
            $(\alpha\ket{\phi})\otimes\ket{\psi}=\alpha(\ket{\phi}\otimes\ket{\psi})$
        \end{center}
        \item Linearity in the second argument:
        \begin{center}
            $\ket{\phi}\otimes(\ket{\psi_1}+\ket{\psi_2})=\ket{\phi}\otimes\ket{\psi_1}+\ket{\phi}\otimes\ket{\psi_2}$\\[1em]
            $\ket{\phi}\otimes(\alpha\ket{\psi})=\alpha(\ket{\phi}\otimes\ket{\psi})$
        \end{center}
    \end{enumerate}
    Notice that scalars "float freely" within tensor products:
    \begin{equation*}
        \alpha\ket{\phi}\otimes\ket{\psi}=\alpha(\ket{\phi}\otimes\ket{\psi})=\ket{\psi}\otimes\alpha\ket{\phi}
    \end{equation*}
    There's no difference multiplying the first argument by the scalar $\alpha$ or multiplying the second argument by the scalar $\alpha$.
}

Tensor products generalize to three or more systems.

\bigbreak

If $\ket{\phi_1},\cdots,\ket{\phi_n}$ are vectors, then the tensor product
\begin{equation*}
    \ket{\psi}=\ket{\phi_1}\otimes\cdots\otimes\ket{\phi_n}
\end{equation*}
is defined by the equation 
\begin{equation*}
    \braket{\alpha_1\cdots\alpha_n\mid\psi}=\braket{\alpha_1\mid\phi_1}\cdots\braket{\alpha_n\mid\phi_n}
\end{equation*}
Equivalently, the tensor product of three or more vectors can be defined recursively:
\begin{equation*}
    \ket{\phi_1}\otimes\cdots\otimes\ket{\phi_n}=(\ket{\phi_1}\otimes\cdots\otimes\ket{\phi_{n-1}})\otimes\ket{\phi_n}
\end{equation*}
We can think about the tensor product of $n$ vectors $\phi_1$ through $\phi_n$ as being the tensor product of the first $n-1$ vectors, followed by the tensor product of the result with the $n$-th vector.\\[1em]
To find the tensor product of the first $n-1$ vectors, we can apply the same recursive definition, and so on until we reach the tensor product of the first two vectors.

\remark{The tensor product of three or more vectors is \textbf{multilinear}.}

\newpage

\subsection{Measurements of probabilistic states}
Measurements of compound systems work in the same way as measurements of single systems.

\qss{}{
    Suppose two systems $(\mathsf{X},\mathsf{Y})$ are together in some probabilistic state. What happens when we measure $\mathsf{X}$ and do nothing to $\mathsf{Y}$?
}

\begin{sol}
    \begin{enumerate}
        \item[]
        \item The probability to observe a particular classical state $\mathsf{x}\in\Sigma$ when just $\mathsf{X}$ is measured is
        \begin{equation*}
            Pr(\mathsf{X}=\mathsf{x})=\sum\limits_{\mathsf{y}\in\Gamma}Pr((\mathsf{X},\mathsf{Y})=(\mathsf{x},\mathsf{y}))
        \end{equation*}
        \item There still may exist uncertainty about the classical state of $\mathsf{Y}$, depending on the outcome of the measurement:
        \begin{equation*}
            Pr(\mathsf{Y}=\mathsf{y}\mid \mathsf{X}=\mathsf{x})=\dfrac{Pr((\mathsf{X},\mathsf{Y})=(\mathsf{x},\mathsf{y}))}{Pr(\mathsf{X}=\mathsf{x})}
        \end{equation*}
    \end{enumerate}
\end{sol}

\raggedright
These formulas can be expressed using the Dirac notation as follows. \\[1em]
Suppose that $(\mathsf{X},\mathsf{Y})$ is in some arbitrary probabilistic state. We start by writing down the probabilistic $(\mathsf{X},\mathsf{Y})$ as a vector:
\begin{equation*}
    \sum\limits_{(\mathsf{x},\mathsf{y})\in\Sigma\times\Gamma}p_{\mathsf{xy}}\ket{\mathsf{xy}}
\end{equation*}
The entries are denoted by $p_{\mathsf{xy}}$ for $\mathsf{x}\in\Sigma$ and $\mathsf{y}\in\Gamma$.\\[1em]
We can then express $\ket{\mathsf{xy}}$ as a tensor product:
\begin{equation*}
    \sum\limits_{(\mathsf{x},\mathsf{y})\in\Sigma\times\Gamma}p_{\mathsf{xy}}\ket{\mathsf{xy}}=\ket{\mathsf{x}}\otimes\ket{\mathsf{y}}
\end{equation*}
We can then use the biliearity of the tensor product, specifically the linearity in the second argument, to write this as:
\begin{equation*}
    \sum\limits_{\mathsf{x}\in\Sigma}\ket{\mathsf{x}}\otimes\left(\sum\limits_{\mathsf{y}\in\Gamma}p_{\mathsf{xy}}\ket{\mathsf{y}}\right)
\end{equation*}
What is happening here is that we are isolating the standard basis vectors on the left hand side, and then grouping the remaining terms on the right hand side. This allows us to see what happens when we measure $\mathsf{X}$ and do nothing to $\mathsf{Y}$.
\begin{enumerate}
    \item The probability that a measurement of $\mathsf{X}$ yields an outcome $\mathsf{x}\in\Sigma$ is:
    \begin{equation*}
        Pr(\mathsf{X}=\mathsf{x})=\sum\limits_{\mathsf{y}\in\Gamma}p_{\mathsf{xy}}
    \end{equation*}
    The probability to get a particular outcome $\mathsf{x}$ is the sum of the probabilities of all the outcomes $(\mathsf{X},\mathsf{Y})$ for $\mathsf{y}\in\Gamma$.
    \item Conditioned on the outcome $\mathsf{x}\in\Sigma$, the probabilistic state of $\mathsf{Y}$ becomes:
    \begin{equation*}
        \dfrac{\sum\limits_{\mathsf{y}\in\Gamma}p_{\mathsf{xy}}\ket{\mathsf{y}}}{\sum\limits_{c\in\Gamma}p_{xc}}
    \end{equation*}
    The probability vector that describes the uncertainty of $\mathsf{Y}$ is the vector that was in the parentheses on the right hand side, except that it has to be normalized for it to be a probability vector.
\end{enumerate}

\newpage

\ex{}{
    Suppose $(\mathsf{X},\mathsf{Y})$ is in the probabilistic state
    \begin{equation*}
        \dfrac{1}{12}\ket{00}+\dfrac{1}{4}\ket{01}+\dfrac{1}{3}\ket{10}+\dfrac{1}{3}\ket{11}
    \end{equation*}
    We write this vector as follows:
    \begin{equation*}
        \ket{0}\otimes\left(\dfrac{1}{12}\ket{0}+\dfrac{1}{4}\ket{1}\right)+\ket{1}\otimes\left(\dfrac{1}{3}\ket{0}+\dfrac{1}{3}\ket{1}\right)
    \end{equation*}
    \textbf{Case 1:} the measurement of $\mathsf{X}$ yields the outcome 0.
    \begin{equation*}
        Pr(\text{outcome is 0})=\dfrac{1}{12}+\dfrac{1}{4}=\dfrac{1}{3}
    \end{equation*}
    Conditioned on this outcome, the probabilistic state of $\mathsf{Y}$ becomes
    \begin{equation*}
        \dfrac{\dfrac{1}{12}\ket{0}+\dfrac{1}{4}\ket{1}}{\dfrac{1}{3}}=\dfrac{1}{4}\ket{0}+\dfrac{2}{3}\ket{1}
    \end{equation*}
    \textbf{Case 2:} the measurement of $\mathsf{X}$ yields the outcome 1.
    \begin{equation*}
        Pr(\text{outcome is 1})=\dfrac{1}{3}+\dfrac{1}{3}=\dfrac{2}{3}
    \end{equation*}
    Conditioned on this outcome, the probabilistic state of $\mathsf{Y}$ becomes
    \begin{equation*}
        \dfrac{\dfrac{1}{3}\ket{0}+\dfrac{1}{3}\ket{1}}{\dfrac{2}{3}}=\dfrac{1}{2}\ket{0}+\dfrac{1}{2}\ket{1}
    \end{equation*}
}

\subsection{Operations on probabilistic states}
Probabilistic operations on compound systems are represented by stochastic matrices having rows and columns that correspond to the Cartesian product of the individual systems' classical state sets.
\ex{CNOT operation}{
    A controlled-NOT (CNOT) operation on two bits $\mathsf{X}$ and $\mathsf{Y}$:\\[1em]
    \hspace{2em} If $\mathsf{X}=1$, then perform a NOT operation on $\mathsf{Y}$, otherwise do nothing.\\[1em]
    $\mathsf{X}$ is the \textbf{control bit} that determines whether a NOT operation is applied to the \textbf{target bit} $\mathsf{Y}$.\\[1em]
    \begin{center}
        Action on standard basis:
        \begin{tabular}{c}
            $\ket{00}\mapsto\ket{00}$ \\
            $\ket{01}\mapsto\ket{01}$ \\
            $\ket{10}\mapsto\ket{11}$ \\
            $\ket{11}\mapsto\ket{10}$
        \end{tabular}
        \hspace{2em}
        Matrix representation:
        $\begin{pmatrix}
            1 & 0 & 0 & 0 \\
            0 & 1 & 0 & 0 \\
            0 & 0 & 0 & 1 \\
            0 & 0 & 1 & 0
        \end{pmatrix}$
    \end{center}
}

\ex{}{
    Here is a different operation on two bits ($\mathsf{X}$,$\mathsf{Y}$):\\[1em]
    \hspace{2em} With probability 1/2, set $\mathsf{Y}$ equal to $\mathsf{X}$, otherwise set $\mathsf{X}$ equal to $\mathsf{Y}$.\\[1em]
    The matrix representation of this operation is as follows:
    \begin{equation*}
        \begin{pmatrix}
            1 & 1/2 & 1/2 & 0 \\
            0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 \\
            0 & 1/2 & 1/2 & 1
        \end{pmatrix}
        =\dfrac{1}{2}
        \begin{pmatrix}
            1 & 1 & 0 & 0 \\
            0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 \\
            0 & 0 & 1 & 1
        \end{pmatrix}
        +\dfrac{1}{2}
        \begin{pmatrix}
            1 & 0 & 1 & 0 \\
            0 & 0 & 0 & 0 \\
            0 & 0 & 0 & 0 \\
            0 & 1 & 0 & 1
        \end{pmatrix}
    \end{equation*}
}

\qss{}{
    Suppose we have two probabilistic operations, each on its own system, described by stochastic matrices:
    \begin{enumerate}
        \item $\mathcal{M}$ is an operation on $\mathsf{X}$.
        \item $\mathcal{N}$ is an operation on $\mathsf{Y}$.
    \end{enumerate}
    If we \textit{simultaneously} perform the two operations, how do we describe the effect on the compound system ($\mathsf{X}$,$\mathsf{Y}$)?\\[1em]
    Hint: if we simultaneously perform the two operations on their own respective systems, then we are in fact performing an operation on the compound system.
}
\begin{sol}
    placeholder
\end{sol}

\subsection{Tensor products of matrices}
The \textbf{tensor product} of two matrices, which is analogous to the tensor product of vectors
\begin{equation*}
    \mathcal{M}=\sum\limits_{\textsf{a,b}\in\Sigma}\alpha_{\textsf{ab}}\ket{\textsf{a}}\bra{\textsf{b}} \quad \text{and} \quad \mathcal{N}=\sum\limits_{\textsf{c,d}\in\Gamma}\beta_{\textsf{cd}}\ket{\textsf{c}}\bra{\textsf{d}}
\end{equation*}
is the matrix
\begin{equation*}
    \mathcal{M}\otimes\mathcal{N}=\sum\limits_{\textsf{a,b}\in\Sigma}\sum\limits_{\textsf{c,d}\in\Gamma}\alpha_{\textsf{ab}}\beta_{\textsf{cd}}\ket{ac}\bra{bd}
\end{equation*}
An alternative, but equivalent, way to define $\mathcal{M}\otimes\mathcal{N}$ is that it is the unique matrix that satisfies the equation
\begin{equation*}
    (\mathcal{M}\otimes\mathcal{N})\ket{\phi\otimes\psi}=\mathcal{M}\ket{\phi}\otimes\mathcal{N}\ket{\psi}
\end{equation*}
for every choice of vectors $\ket{\phi}$ and $\ket{\psi}$.
\remark{One way to think about this is that the tensor product of matrices works perfectly with the tensor product of vectors. This is, of course, an equation that you would hope is true.}

We can define the tensor product in terms of matrices written out:
\begin{center}
    $\begin{pmatrix}
        \alpha_{11} & \cdots & \alpha_{1m} \\
        \vdots & \ddots & \vdots \\
        \alpha_{m1} & \cdots & \alpha_{mm}
    \end{pmatrix}
    \otimes
    \begin{pmatrix}
        \beta_{11} & \cdots & \beta_{1k} \\
        \vdots & \ddots & \vdots \\
        \beta_{k1} & \cdots & \beta_{kk}
    \end{pmatrix}$
    $=
    \begin{pmatrix}
        \alpha_{11}\beta_{11} & \cdots & \alpha_{11}\beta_{1k} &  & \alpha_{1m}\beta_{11} & \cdots & \alpha_{1m}\beta_{1k} \\
        \vdots & \ddots & \vdots & \cdots & \vdots & \ddots & \vdots \\
        \alpha_{11}\beta_{k1} & \cdots & \alpha_{11}\beta_{kk} &  & \alpha_{1m}\beta_{k1} & \cdots & \alpha_{1m}\beta_{kk} \\
        & \vdots &  & \ddots &  & \vdots &  \\
        \alpha_{m1}\beta_{11} & \cdots & \alpha_{m1}\beta_{1k} &  & \alpha_{mm}\beta_{11} & \cdots & \alpha_{mm}\beta_{1k} \\
        \vdots & \ddots & \vdots & \cdots & \vdots & \ddots & \vdots \\
        \alpha_{m1}\beta_{k1} & \cdots & \alpha_{m1}\beta_{kk} &  & \alpha_{mm}\beta_{k1} & \cdots & \alpha_{mm}\beta_{kk}
    \end{pmatrix}$
\end{center}